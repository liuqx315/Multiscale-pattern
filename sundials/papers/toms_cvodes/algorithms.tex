\section{Algorithms}\label{s:algorithms}

CVODES solves initial value problems with free parameters. 
Such problems can be stated as
\begin{equation}\label{e:ivp}
\dot{y}  = f(t,\,y,\,p) \, , \quad y(t_0)  = y_0(p) \, ,
\end{equation}
where $y \in {\bf R}^N$ is  the vector of state variables, 
$\dot{y}\,=dy/dt$, and $p \in {\bf R}^{N_p}$ are problem parameters.
Additionally, CVODES can also compute first order derivative
information, performing either
{\em forward sensitivity analysis} or {\em adjoint sensitivity analysis}.
In the first case, CVODES computes the sensitivities of the solution
with respect to the parameters $p$, while in the second case, CVODES
computes the gradient of a {\em derived function} with respect to the
parameters $p$.

In the rest of this section we describe the algorithms implemented in
CVODES, with emphasis on sensitivity analysis. We give only a brief
overview of the ODE integration algorithm to introduce some of the
quantities needed in the sequel.  Since CVODES shares its main
integration algorithm with CVODE, the interested reader is directed
to~\cite{HBGLSSW:04}.

%-------------------------------------------------------------------------------

\subsection{ODE Integration}\label{ss:integration}

CVODES solves the IVP using either Backward Differentiation Formula
(BDF) methods or Adams-Moulton methods.  Both are implemented with
dynamically varying stepsize and order, based on the control of local
errors to meet user-specified tolerances.  A central feature of the
method is the solution, at each time step, of a nonlinear system of
size $N$, of the form
\begin{equation}\label{e:nonlinear}
    F(y_n) \equiv y_n - \gamma_n f(t_n,\,y_n,\,p) - a_n = 0 \,
\end{equation}
where $\gamma_n$ is a scalar and $a_n$ is a constant vector.
This system is solved by either functional (fixpoint) iteration or
some form of Newton iteration.  In the latter case, the matrix in the
linear system for Newton corrections has the form $M = I-\gamma_nJ_n$,
where $J_n = {\partial f}/{\partial y}$ at $(t_n,y_n)$.

CVODES also incorporates an algorithm for special treatment of
quadratures depending on the solution $y$ of (\ref{e:ivp}).
An efficient quadrature computation is needed in the context of
adjoint sensitivity analysis (see Section~\ref{ss:adj_sensitivity}).
Evaluation of integrals of the form
$G = \int_{t_0}^{t_\mb{f}} g(t,y,p) dt$ can be done efficiently using the
underlying linear multistep method interpolating polynomials by
appending to (\ref{e:ivp}) an additional ODE
$\dot\phi = g(t,y,p)$, with initial condition $\phi(t_0) = 0$,
in which case $G = \phi(t_f)$. In the context of an implicit ODE
integrator, since the right-hand side of this additional equation does not
depend on $\phi$, such equations need not participate in the solution of
the nonlinear system~(\ref{e:nonlinear}). CVODES allows the user to
identify these equations separately from those in~(\ref{e:ivp}) and
provides the option of including or excluding $\phi$ from the error
control algorithm.
A similar treatment of quadratures is included in the DASPK3
code~\cite{LiPe:99a,LiPe:00}.


A complete description of the CVODES integration algorithm, including 
the nonlinear solver convergence, error control mechanism, and heuristics 
related to stopping criteria and finite-difference parameter selection, is
given in~\cite{HBGLSSW:04}.

%-------------------------------------------------------------------------------

\subsection{Forward Sensitivity Analysis}\label{ss:fwd_sensitivity}

Typically, the governing equations of complex, large-scale models
depend on various parameters,  through the right-hand side vector 
and/or through the vector of initial conditions, as in (\ref{e:ivp}).
In addition to numerically solving the ODEs, it may be desirable to
determine the sensitivity of the results with respect to the model
parameters. 
Such sensitivity information can be used to estimate which
parameters are most influential in affecting the behavior of the
simulation or to evaluate optimization gradients (in the setting of dynamic
optimization, parameter estimation, optimal control, etc.).

The {\em solution sensitivity} with respect to the model parameter
$p_i$ is defined as the vector 
$s_i (t) = {\partial y(t)}/{\partial p_i}$
and satisfies the following {\em forward sensitivity equations}
(or in short {\em sensitivity equations}):
\begin{equation}\label{e:sens_eqns}
\dot{s_i}  = \frac{\partial f}{\partial y} s_i + \frac{\partial f}{\partial p_i} \, ,
\quad s_i(t_0)  = \frac{\partial y_{0}(p)}{\partial p_i} \, ,
\end{equation}
obtained by applying the chain rule of differentiation to the original
ODEs~(\ref{e:ivp}). 

When performing forward sensitivity analysis, CVODES carries out the time integration 
of the combined system, (\ref{e:ivp}) and (\ref{e:sens_eqns}), by viewing it as an ODE
system of size $N(N_s+1)$, where $N_s$ represents a subset of model parameters $p_i$, 
with respect to which sensitivities are desired ($N_s \le N_p$). 
The sensitivity equations are solved with the same linear multistep formula that
was selected for the original ODEs and, if Newton iteration was selected, the
same linear solver is used in the correction phase for both state and sensitivity 
variables. In addition, CVODES offers the option of including
({\em full error control}) or excluding
({\em partial error control}) the sensitivity variables from the local 
error test.

\subsubsection{Forward sensitivity methods}
In what follows we briefly describe three methods that have been proposed for the 
solution of the combined ODE and sensitivity system for the vector
${\hat y} = [y, s_1, \ldots , s_{N_s}]$.

\begin{itemize}

\item[{\em Staggered Direct.}]
  In this approach \cite{CaSt:85}, the nonlinear system (\ref{e:nonlinear}) is first 
  solved and, once an acceptable numerical solution is obtained, the sensitivity 
  variables at the new step are found by directly solving (\ref{e:sens_eqns}) 
  after the BDF discretization is used to eliminate ${\dot s}_i$. 
  Although the system matrix of the above linear system is based on exactly the same 
  information as the matrix $M$, it must be updated and factored 
  at every step of the integration, in contrast to $M$ which is updated only ocasionally. 
  For problems with many parameters (relative to the problem size), the staggered direct 
  method can outperform the methods described below~\cite{LPZ:99}.
  However, the computational cost associated with matrix updates and factorizations 
  makes this method unattractive for problems with many more states than parameters
  (such as those arising from semidiscretization of PDEs).
  
\item[{\em Simultaneous Corrector.}] 
  In this method \cite{MaPe:96}, the BDF discretization is applied simultaneously
  to both the original equations (\ref{e:ivp}) and the sensitivity systems
  (\ref{e:sens_eqns}) resulting in an extended nonlinear system 
  %%   \begin{equation*}
  %%     {\hat G}({\hat y}_n) \equiv  
  %%     {\hat y}_n - h_n\beta_{n,0} {\hat f}(t_n,\,{\hat y}_n) - {\hat a}_n = 0 \, ,
  %%   \end{equation*}
  %%   where
  %%   ${\hat f} = [ f(t,y,p), \ldots , (\dfdyI)(t,y,p) s_i + (\dfdpiI)(t,y,p) , \ldots ]$
  %%   and ${\hat a}_n$ are the terms in the BDF discretization that depend on the
  %%   solution at previous integration steps.
  in the uknown ${\hat y} = [y, s_1, \ldots , s_{N_s}]$.
  This combined nonlinear system can be solved using either functional or
  Newton iteration. In the latter case, Maly and Petzold have shown
  that 2-step quadratic convergence can be attained with a modified Newton scheme
  which uses only the block-diagonal portion of the iteration matrix.
  This results in a decoupling that allows the reuse of 
  $M$ without additional matrix factorizations. However, the products
  $(\dfdyI)s_i$ as well as the vectors $\dfdpiI$ must still be reevaluated at 
  each step of the iterative process to update the 
  sensitivity portions of the residual.
  
\item[{\em Staggered corrector.}] In this approach \cite{FTB:97}, as in the staggered direct method,
  the nonlinear system (\ref{e:nonlinear}) is solved first for the state variables.
  Then, a separate nonlinear iteration is used to solve the sensitivity system.
  In this approach, the vectors $\dfdpiI$ need be updated only once per integration step, 
  after the state correction phase has converged. 
  When using functional iteration, this amounts to using a stationary iterative method for
  the linear system (\ref{e:sens_eqns}). When using Newton iteration, this amounts to 
  using a modified-Newton iteration to solve the linear system (\ref{e:sens_eqns}), 
  the Newton iteration matrix being only an approximation to the system matrix.
  An important observation is that the staggered corrector method, combined with 
  Newton iterations and the SPGMR linear solver, effectively results in a staggered direct 
  method~\cite{Toc:01}.
  Indeed, SPGMR requires only the action of the matrix $M$ on a vector and
  this can be provided with up-to-date Jacobian information. Therefore, the
  modified Newton procedure will theoretically converge 
  after one iteration.

\end{itemize}  
%%
CVODES implements the simultaneous corrector method and two flavors of the 
staggered corrector method which differ only if the sensitivity variables are
included in the error control test.
In the {\em full error control} case, 
the first variant of the staggered corrector method requires the convergence of 
the nonlinear sensitivity iterations for all $N_s$ sensitivity sytems and then 
performs the error test on the sensitivity variables. The second variant of the method
will perform the error test for each sensitivity vector $s_i$ ($i=1,\ldots,N_s$),
individually, as they pass the convergence test. Differences in performance
between the two variants may therefore be noticed whenever one of the sensitivity 
vectors $s_i$ fails a convergence or error test. 

We note that the DASPK3.0 code \cite{LiPe:99b,LiPe:99a} implements the staggered
direct, simultaneous corrector, and staggered corrector methods. The code
DSL48S \cite{FTB:97,Fee:98} also contains the staggered corrector method.

\subsubsection{Selection of the absolute tolerances for sensitivity variables}
If the sensitivities are included in the error test, CVODES provides an 
automated estimation of absolute tolerances for the sensitivity variables 
based on the absolute tolerance for the corresponding state variable.
The relative tolerance for sensitivity variables is set to be the same as for 
the state variables. The selection of absolute tolerances for the sensitivity 
variables is based on the observation that the sensitivity vector $s_i$ will have 
units of $[y]/[p_i]$.
With this, the absolute tolerance for the $j$-th component of the sensitivity
vector $s_i$ is set to ${\mbox{\sc atol}_j}/{|{\bar p}_i|}$,
where $\mbox{\sc atol}_j$ are the absolute tolerances for the state variables and $\bar p$
is a vector of scaling factors that are dimensionally consistent with
the model parameters $p$ and give indication of their order of magnitude.
This choice of relative and absolute tolerances is equivalent 
to requiring that the weighted root-mean-square norm of the sensitivity 
vector $s_i$ with weights based on $s_i$ is the same as the
weighted root-mean-square norm of the vector of scaled sensitivities 
${\bar s}_i = |{\bar p}_i| s_i$ with weights based on the state variables
(the scaled sensitivities ${\bar s}_i$ being dimensionally consistent with the
state variables).
%
However, this choice of tolerances for the $s_i$ may be a poor one, and the user 
of CVODES can provide different values as an option.

\subsubsection{Evaluation of the sensitivity right-hand side}
There are several methods for evaluating the right-hand side of the sensitivity 
systems (\ref{e:sens_eqns}): analytic evaluation, automatic differentiation, 
complex-step approximation, and finite differences (or directional derivatives).

Since it allows for user-defined functions for the evaluation of any and all 
derivative information, CVODES provides all the software hooks for implementing 
interfaces to automatic differentiation or complex-step approximation. 
We have prototyped an automated code generator tool (not included in
the CVODES distribution) which parses C code and generates C++ code to
perform complex arithmetic. The user's right-hand side function can
thus be transformed to allow for the automatic generation of
derivative information using complex-step approximations~\cite{MSA:03}.
This approach allows for very accurate numerical estimation of
derivatives as it circumvents the subtraction cancellation error
typical for finite difference methods. However, any code
transformation approach to the automatic generation of derivative
information from C functions (for example ADIC~\cite{BRM:97})
has the disadvantage of requiring transformations on the user's data
structures, which are otherwise treated as black boxes by CVODES.
This makes the design of general interfaces a challenging task, but we 
are investigating avenues to overcome this issue.

The default option in CVODES for the evaluation of sensitivity right-hand 
sides is to use finite difference-based approximations for the terms $(\dfdyI) s_i$ 
and $\dfdpiI$, or directional derivatives to evaluate
$(\dfdyI) s_i + \dfdpiI$.
As is typical for finite differences, the proper choice of perturbations is a 
delicate matter. CVODES takes into account several problem-related features:
the relative ODE error tolerance $\mbox{\sc rtol}$, the machine unit roundoff $U$,
the scale factor ${\bar p}_i$, and the weighted root-mean-square norm of the 
sensitivity vector $s_i$.

Using central finite differences as an example, the two terms 
$({\partial f}/{\partial y}) s_i$ 
and ${\partial f}/{\partial p_i}$ in the right-hand side of (\ref{e:sens_eqns})
can be evaluated separately:
\begin{gather}
  \frac{\partial f}{\partial y} s_i \approx \frac{f(t, y+\sigma_y s_i, p)-
    f(t, y-\sigma_y s_i, p)}{2\,\sigma_y} \, , \label{e:fd2} \\
  \frac{\partial f}{\partial p_i} \approx \frac{f(t,y,p + \sigma_i e_i)-
    f(t,y,p - \sigma_i e_i)}{2\,\sigma_i} \, , \tag{\ref{e:fd2}'} \\
  \sigma_i = |{\bar p}_i| \sqrt{\max( \mbox{\sc rtol} , U)} \, , \quad
  \sigma_y = \frac{1}{\max(1/\sigma_i, \|s_i\|_{\mbox{\scriptsize WRMS}}
                                          /|{\bar p}_i|)} \, , \nonumber
\end{gather}
simultaneously:
\begin{gather}
  \frac{\partial f}{\partial y} s_i + \frac{\partial f}{\partial p_i} \approx
  \frac{f(t, y+\sigma s_i, p + \sigma e_i) -
    f(t, y-\sigma s_i, p - \sigma e_i)}{2\,\sigma} \, , \label{e:dd2} \\
  \sigma = \min(\sigma_i, \sigma_y) \, , \nonumber
\end{gather}
or adaptively switching between (\ref{e:fd2})+(\ref{e:fd2}') and (\ref{e:dd2}), 
depending on the relative size of the estimated finite difference 
increments $\sigma_i$ and $\sigma_y$.

%-------------------------------------------------------------------------------

\subsection{Adjoint Sensitivity Analysis}\label{ss:adj_sensitivity}

In the {\em forward sensitivity approach} described in the previous
section, obtaining sensitivities with respect to $N_s$ parameters is roughly
equivalent to solving an ODE system of size $(1+N_s) N$. This can become 
prohibitively expensive, especially for large-scale problems, if sensitivities
with respect to many parameters are desired.
In this situation, the {\em adjoint sensitivity method} is a very
attractive alternative, provided that we do not need the solution sensitivities
$s_i$, but rather the gradients with respect to model parameters of a relatively 
few derived functionals of the solution. In other words, if $y(t)$ is the solution
of (\ref{e:ivp}), we wish to evaluate the gradient ${dG}/{dp}$ of
\begin{equation}\label{e:G}
G(p) = \int_{t_0}^{t_\mb{f}} g(t, y, p) dt \, ,
\end{equation}
or, alternatively, the gradient ${dg}/{dp}$ of the function $g(t, x, p)$ 
at time $t_\mb{f}$. 
The function $g$ must be smooth enough that $\partial g / \partial y$ 
and $\partial g / \partial p$ exist and are bounded. 
%
The gradient of $G$ with respect to $p$ is simply
\begin{equation}\label{e:dGdp}
  \frac{dG}{dp} = \lambda^T(t_0) s(t_0) + 
  \int_{t_0}^{t_\mb{f}} \left( \frac{\partial g}{\partial p} + 
    \lambda^T \frac{\partial f}{\partial p} \right) dt,
\end{equation}
where $\lambda$ is solution of
\begin{equation}\label{e:adj_eqns}
{\dot \lambda} = -\left( \dfdy \right)^T \lambda - 
\left( \frac{\partial g}{\partial y} \right)^T \, ,
\quad \lambda(t_\mb{f}) = 0
\end{equation}
and $s(t_0) = \partial y_0 / \partial p$.
%
The gradient of $g(t_\mb{f},y,p)$ with respect to $p$ can be then obtained
by using the Leibnitz differentiation rule \cite{CLPS:03}.

The first thing to notice about the adjoint system (\ref{e:adj_eqns})
is that there is no explicit specification of the parameters $p$; this
implies that, once the solution $\lambda$ is found, the formula
(\ref{e:dGdp}) can then be used to find the gradient of $G$ with
respect to any of the parameters $p$.
The second important remark is that the adjoint systems are terminal
value problems which depend on the solution $y(t)$ of the original IVP
(\ref{e:ivp}).  Therefore, a procedure is needed for providing the
states $y$ obtained during a forward integration phase of
(\ref{e:ivp}) to CVODES during the backward integration phase of
(\ref{e:adj_eqns}).
The approach adopted in CVODES, similar to that implemented
in DASPKADJOINT \cite{LiPe:00}, is justified below.

Since CVODES implements variable-stepsize integration formulas,
it is unlikely that the states will be available at the desired time and
therefore some form of interpolation is needed. The CVODES implementation
being also variable-order, it is possible that during the forward
integration phase the order may be reduced as low as first order,
which means that there may be points in time where only $y$ and ${\dot y}$
are available. Therefore, CVODES employs a cubic Hermite interpolation
algorithm. However, especially for large-scale problems and long integration
intervals, the number and size of the vectors $y$ and ${\dot y}$ that would 
need to be stored make this approach computationally intractable. 

CVODES settles for a compromise between storage space and execution
time by implementing a check-pointing scheme which, at the cost of at
most one additional forward integration, offers the best possible
estimate of memory requirements for adjoint sensitivity analysis.
Note that truly optimal checkpointing \cite{GrWa:00} cannot be used
since the number of integration steps is not known apriori.

%% To begin with,
%% based on the problem size $N$ and the available memory, the user decides on 
%% the number $N_d$ of data pairs ($y$, ${\dot y}$) that can be kept in memory for 
%% the purpose of interpolation. Then, during the first forward integration stage, 
%% every $N_d$ integration steps a check point is formed by saving enough information
%% (either in memory or on disk if needed) to allow for a hot restart, that is, a restart
%% that will exactly reproduce the forward integration. In order to avoid storing
%% Jacobian-related data at each check point, a reevaluation of the iteration matrix
%% is forced before each check point.
%% The backward integration from check point $i+1$ to check point $i$ is preceded
%% by a forward integration from $i$ to $i+1$ during which $N_d$ data pairs 
%% ($y$, ${\dot y}$) are generated and stored in memory for interpolation.
%% This procedure is illustrated in Fig.~\ref{f:ckpnt}.
%
\begin{figure}
\centerline{\psfig{figure=ckpnt.eps,width=4in}}
\caption {Illustration of the check-pointing algorithm for generation of 
  the forward solution during the integration of the adjoint system.}
\label{f:ckpnt}
\end{figure}
%%
This approach, illustrated in Fig.~\ref{f:ckpnt}, transfers the uncertainty in the number of integration
steps in the forward integration phase to uncertainty in the final number of check 
points. However, $N_c$ is much smaller than the number of steps taken during
the forward integration, and there is no major penalty for writing and then reading
check point data to/from a temporary file.
%

We note that the adjoint sensitivity module in CVODES provides the 
infrastructure to integrate backwards in time any ODE terminal value problem
dependent on the solution of the IVP (\ref{e:ivp}), including
the adjoint system (\ref{e:adj_eqns}), as well as any other
quadrature ODEs that may be needed in evaluating the integral in (\ref{e:dGdp}).
In particular, for ODE systems arising from semi-discretization
of time-dependent PDEs, this feature allows for integration either of the 
discretized adjoint PDE system or of the adjoint of the discretized PDE,
since these two formulations are not equivalent~\cite{ArSa:97,LiPe:04}.

Finally, we mention that, when using the backward integration module for adjoint
sensitivity analysis, the CVODES interface allows for user-provided
functions for the evaluation of the adjoint systems that are generated through
reverse automatic differentiation. Due to the current development stage of
reverse AD tools for C codes, CVODES cannot provide generic wrappers
(as done, for example, in DASPKADJOINT for Fortran77 codes). At this time, the
burden of interfacing CVODE with AD-generated functions must rely on
the user.
