\subsection{KINSOL}\label{ss:kinsol}

KINSOL solves nonlinear algebraic systems in real space, which we write as
\begin{equation}\label{nonlinear_system}
  F(u) = 0 \, , \quad F:\mbox{\bf R}^N \rightarrow \mbox{\bf R}^N \, ,
\end{equation}
given an initial guess $u_0$.  It is a rewrite in C of the Fortran 77
code NKSOL of Brown and Saad \cite{BrSa:90}.

KINSOL employs the Inexact Newton method developed in
\cite{BrSa:90,Bro:87,DES:82}
and further described in \cite{DeSc:96,Kel:95},
resulting in the following iteration:

\vspace{1ex}
\noindent{\bf Inexact Newton iteration}
\begin{enumerate}
   \item Set $u_0 = $ an initial guess
   \item For $n = 0, 1, 2,...$ until convergence do:
      \begin{itemize}
          \item[(a)] Approximately solve $J(u_n)\delta_n = - F(u_n)$
          \item[(b)] Set $u_{n+1} = u_n + \lambda \delta_n$,
          $\lambda \leq 1$
          \item[(c)] Test for convergence
      \end{itemize}
\end{enumerate}
Here, $u_n$ is the $n$th iterate to $u$, and $J(u) = F'(u)$ is the
system Jacobian. As this code module is anticipated for use on
large systems, only iterative methods are provided to solve the
system in step 2(a). These solutions are only approximate.  At
each stage in the iteration process, a scalar multiple of the
approximate solution, $\delta_n$, is added to $u_n$ to produce a
new iterate, $u_{n+1}$. A test for convergence is made before the
iteration continues.

The linear iterative method currently implemented is one of the class of
Krylov methods, GMRES \cite{BrHi:89,SaSc:86},
provided through the SPGMR module common to all SUNDIALS codes.
Use of SPGMR provides a linear solver which, by default, is applied in a
matrix-free manner, with matrix-vector products $Jv$ obtained by either
finite difference quotients or a user-supplied routine.
In the case where finite differences are used,
the matrix-vector product $J(u)v$ is approximated by a quotient of the form
given in (\ref{jacobv}),
where $f(t, y) = F(y)$ for our nonlinear system,
$u$ is the current approximation to a root of (\ref{nonlinear_system}),
and $\sigma$ is a scalar.  The choice of $\sigma$ is taken from
\cite{BrSa:90} and is given by
\begin{equation}\label{sigma_comp}
  \sigma = \frac{\max \{|u^T v|, \mbox{typ}u^T |v|\}}{\|v\|_2}
  \mbox{sign}(u^T v) \sqrt{U} \, ,
\end{equation}
where $\mbox{typ}u$ is a vector of typical values for the absolute
values of the solution (and can be taken to be inverses of the scale
factors given for $u$ as described below), and $U$ is unit roundoff.
Convergence of the Newton method is maintained as long as the value of
$\sigma$ remains appropriately small as shown in \cite{Bro:87}.

To the above methods are added scaling and preconditioning.
Scaling is allowed for both the solution vector and the system
function vector. For scaling to be used, the user should supply
values $D_u$, which are diagonal elements of the scaling matrix
such that $D_u u_n$ has all components roughly the same magnitude
when $u_n$ is close to a solution, and $D_F F$ has all components
roughly the same magnitude when $u_n$ is not too close to a
solution. In the text below, we use the following scaled norms:
\begin{equation}
\|z\|_{D_u} = \|D_u z\|_2, \;\; \|z\|_{D_F} = \|D_F z\|_2, \;\;
{\rm and} \;\; \|z\|_{D,\infty} = \|Dz\|_{\infty},
\end{equation}
where $\| \cdot \|_{\infty}$ is the max norm.  When scaling values
are provided for the solution vector, these values are
automatically incorporated into the calculation of $\sigma$ in
(\ref{sigma_comp}). Additionally, right preconditioning is
provided if the preconditioning setup and solve routines are
supplied by the user. In this case, GMRES is applied to the linear
systems $(JP^{-1})(P\delta) = -F$.

Two methods of applying a computed step $\delta_n$ to the
previously computed solution vector are implemented. The first and
simplest is the Inexact Newton strategy which applies step 2(b) as
above with $\lambda$ always set to $1$. The other method is a
global strategy, which attempts to use the direction implied by
$\delta_n$ in the most efficient way for furthering convergence of
the nonlinear problem. This technique is implemented in the second
strategy, called Linesearch.  This option employs both the
$\alpha$ and $\beta$ conditions of the Goldstein-Armijo linesearch
given in \cite{DeSc:96} for step 2(b), where $\lambda$ is chosen
to guarantee a sufficient decrease in $F$ relative to the step
length as well as a minimum step length relative to the initial
rate of decrease of $F$.  One property of the algorithm is that
the full Newton step tends to be taken close to the solution.  For
more details, the reader is referred to \cite{DeSc:96}.

Stopping criteria for the Newton method can be required for either or
both of the nonlinear residual and the step length.  For the former,
the Newton iteration must pass a stopping test
\[ \|F(u_n)\|_{D_F,\infty} < \mbox{\sc ftol} \, , \]
where {\sc ftol} is an input scalar tolerance with a default value of
$U^{1/3}$.
For the latter, the Newton method will terminate when the maximum scaled step
is below a given tolerance
\[ \|\delta_n\|_{D_u,\infty} < \mbox{\sc steptol} \, , \]
where {\sc steptol} is an input scalar tolerance with a default value of
$U^{2/3}$.

Three options for stopping criteria for the linear system solve are
implemented, including the two
algorithms of Eisenstat and Walker \cite{EiWa:96}.
The Krylov iteration must pass a stopping test
\[ \|J \delta_n + F\|_{D_F} < (\eta_n + U) \|F\|_{D_F} \, , \]
where $\eta_n$ is one of:
\begin{itemize}
\item Eisenstat and Walker Choice 1
  \[
  \eta_n = \frac{\left|\; \|F(u_n)\|_{D_F}
      - \|F(u_{n-1}) + J(u_{n-1}) \delta_n \|_{D_F}
      \; \right|}
  {\|F(u_{n-1})\|_{D_F}} \, ,
  \]
\item Eisenstat and Walker Choice 2
  \[
  \eta_n = \gamma
  \left( \frac{ \|F(u_n)\|_{D_F}}{\|F(u_{n-1})\|_{D_F}} \right)^{\alpha} \, ,
  \]
where default values of $\gamma$ and $\alpha$ are $0.9$ and $2$,
 respectively.
\item  $\eta_n$ = constant with 0.1 as the default.
\end{itemize}
The default is Eisenstat and Walker Choice 1. For both options 1 and 2,
appropriate safeguards are incorporated to ensure that $\eta$ does not
decrease too fast~\cite{EiWa:96}.

As a user option, KINSOL permits the application of inequality
constraints, $u^i > 0$ and $u^i < 0$, as well as $u^i \geq 0$ and
$u^i \leq 0$, where $u^i$ is the $i$th component of $u$.  Any such
constraint, or no constraint, may be imposed on each component.
KINSOL will reduce step lengths in order to ensure that no
constraint is violated.  Specifically, if a new Newton iterate
will violate a constraint, the maximum (over all $i$) step length
along the Newton direction that will satisfy all constraints is
found and $\delta_n$ in Step 2(b) is scaled to take a step of that
length.
