\subsection{CVODE}\label{ss:cvode}

% What does CVODE do?
%--------------------
CVODE solves ODE initial value problems in real $N$-space.  We
write such problems in the form
\begin{equation}\label{ODE}
  \dot{y} = f(t,y) \, ,\quad y(t_0) = y_0 \, ,
\end{equation}
where $y \in \mbox{\bf R}^N$.
Here we use $\dot{y}$ to denote $dy/dt$.  While we use $t$ to denote
the independent variable, and usually this is time, it certainly need
not be.  CVODE solves both stiff and nonstiff systems.  Roughly
speaking, stiffness is characterized by the presence of at least one
rapidly damped mode, whose time constant is small compared to the time
scale of the solution itself.  (See \cite{HaWa:91} for more on stiffness.)

% Linear multistep methods
%-------------------------
The methods used in CVODE are variable-order, variable-step multistep
methods, based on formulas of the form
\begin{equation*}
 \sum_{i = 0}^{K_1} \alpha_{n,i} y_{n-i} +
     h_n \sum_{i = 0}^{K_2} \beta_{n,i} \dot{y}_{n-i} = 0 \, .
\end{equation*}
Here the $y_n$ are computed approximations to $y(t_n)$, and $h_n = t_n -
t_{n-1}$ is the step size.  The user of CVODE must choose appropriately from
one of two families of multistep formulas.  For nonstiff problems, CVODE
includes the Adams-Moulton formulas, characterized by $K_1 = 1$ and $K_2 =
q$ above, where the order $q$ varies between $1$ and $12$.  For stiff
problems, CVODE includes the Backward Differentiation Formulas (BDFs) in
so-called fixed-leading coefficient form, given by $K_1 = q$ and $K_2 = 0$,
with order $q$ varying between $1$ and $5$.  The coefficients are uniquely
determined by the method type, its order, the recent history of the step
sizes (the last $q$ values), and the normalization $\alpha_{n,0} = -1$.  See
\cite{ByHi:75} and
\cite{JaSD:80}.

% Nonlinear system
%-----------------
For either choice of formula, the nonlinear system
\begin{equation}\label{NLS}
  G(y_n) \equiv y_n - h_n \beta_{n,0} f(t_n,y_n) - a_n = 0 \, ,
\end{equation}
where $a_n\equiv\sum_{i>0}(\alpha_{n,i}y_{n-i}+h_n\beta_{n,i}\dot{y}_{n-i})$,
must be solved (approximately) at each integration step.  For this, CVODE
offers the choice of either {\em functional iteration}, suitable only
for nonstiff systems, and various versions of {\em Newton iteration}.
If we denote the Newton iterates by $y_{n,m}$, then functional iteration,
given by
\[ y_{n,m+1} = h_n \beta_{n,0} f(t_n,y_{n,m}) + a_n \, , \]
involves evaluations of $f$ only.  In contrast, Newton iteration requires
the solution of linear systems
\begin{equation*}
  M [y_{n,m+1} - y_{n,m}] = -G(y_{n,m}) \, ,
\end{equation*}
in which
\begin{equation}\label{Newtonmat}
  M \approx I - \gamma J \, ,
  \quad J = \partial f / \partial y \, ,
  \quad \mbox{and} \quad
  \gamma = h_n \beta_{n,0} \, .
\end{equation}
In either case, the initial guess for the iteration is a predicted
value $y_{n,0}$ computed explicitly from the available history data
(the last $q+1$ computed values of $y$ or $\dot{y}$).
For the Newton corrections, CVODE provides a choice of four methods:
\begin{itemize}
\item a dense direct solver (serial version only),
\item a band direct solver (serial version only),
\item a diagonal approximate Jacobian solver \cite{RaHi:93}, or
\item SPGMR = Scaled Preconditioned GMRES, without restarts
\cite{BrHi:89}.
\end{itemize}
(By ``serial version'' we mean the CVODE solver with the serial
NVECTOR module attached.)

For large stiff systems, where direct methods are not feasible, the
combination of a BDF integrator with the SPGMR algorithm
yields a powerful tool because it combines established methods for
stiff integration, nonlinear iteration, and Krylov (linear) iteration
with a problem-specific treatment of the dominant source of stiffness,
in the form of the user-supplied preconditioner matrix \cite{BrHi:89}.

% WRMS Norm
%----------
In the process of controlling errors at various levels, CVODE uses a
weighted root-mean-square norm, denoted
$\|\cdot\|_{\mbox{\scriptsize WRMS}}$, for all
error-like quantities:
\begin{equation}\label{wrmsnorm}
 \|v\|_{\mbox{\scriptsize WRMS}} = \sqrt{N^{-1}\sum_1^N (v_i/W_i)^2}
 \, .
\end{equation}
The weights $W_i$ are based on the current solution (with components
denoted $y^i$), and on the relative tolerance {\sc rtol} and absolute
tolerances {\sc atol}$_i$ input by the user, namely
\begin{equation}\label{errwt}
 W_i = \mbox{\sc rtol} \cdot |y^i| + \mbox{\sc atol}_i \, .
\end{equation}
Because $W_i$ represents a tolerance in the component $y^i$, a vector
representing a perturbation in $y$ and having norm of 1 is regarded as
``small.''  For brevity, we will usually drop the subscript WRMS on
norms in what follows.

% Newton iteration
%-----------------
In the cases of a direct solver (dense, band, or diagonal), the
iteration is a Modified Newton iteration, in that the iteration matrix
$M$ is fixed throughout the nonlinear iterations.  However, for SPGMR,
it is an Inexact Newton iteration, in which $M$ is applied in a
matrix-free manner, with matrix-vector products $Jv$ obtained by
either difference quotients or a user-supplied routine.  The matrix
$M$ (direct cases) or preconditioner matrix $P$ (SPGMR case) is
updated as infrequently as possible, to balance the high costs of
matrix operations against other costs.  Specifically, this matrix
update occurs when:
\begin{itemize}
\item starting the problem,
\item more than 20 time steps have been taken since the last update,
\item the current value of $\gamma$ and its value at the last update
($\bar{\gamma}$) satisfy $|\gamma/\bar{\gamma} - 1| > 0.3$,
\item a convergence failure just occurred, or
\item an error test failure just occurred.
\end{itemize}
When forced by a convergence failure, an update of $M$ or $P$ may or
may not involve a re-evaluation of $J$ (in $M$) or of Jacobian data
(in $P$), depending on whether Jacobian error was the likely cause of
the failure.  More generally, the decision is made to re-evaluate $J$
(or instruct the user to re-evaluate Jacobian data in $P$) when:
\begin{itemize}
\item starting the problem,
\item more than 50 steps have been taken since the last evaluation,
\item a convergence failure occurred with an outdated matrix, and
the value $\bar{\gamma}$ of $\gamma$ at the last update
satisfies $|\gamma/\bar{\gamma} - 1| < 0.2$, or
\item a convergence failure occurred that forced a step size reduction.
\end{itemize}

% Newton convergence test
%------------------------
The stopping test for the Newton iteration is related to the
subsequent local error test, with the goal of keeping the nonlinear
iteration errors from interfering with local error control.  The final
computed iterate $y_{n,m}$ will have to satisfy a local error test
$\|y_{n,m} - y_{n,0}\| \leq \epsilon$, where $\epsilon$ is an error
test constant described below.  Letting $y_n$ denote the exact
solution of (\ref{NLS}), we want to ensure that the iteration error
$y_n-y_{n,m}$ is small relative to $\epsilon$, specifically that it is
less than $0.1 \epsilon$.  (The safety factor $0.1$ can be changed by
the user.)  For this, we also estimate the linear convergence rate
constant $R$ as follows.  We initialize $R$ to 1, and reset $R = 1$
when $M$ or $P$ is updated.  After computing a correction
$\delta_m = y_{n,m} - y_{n,m-1}$, we update $R$ if $m > 1$ as
\begin{equation*}
  R \leftarrow \max\{0.3R , \|\delta_m\| / \|\delta_{m-1}\| \} \, ,
\end{equation*}
and we use the estimate
\begin{equation*}
  \| y_n - y_{n,m} \| \approx \| y_{n,m+1} - y_{n,m} \|
  \approx R \| y_{n,m} - y_{n,m-1} \|  =  R \|\delta_m \| \, .
\end{equation*}
Therefore the convergence (stopping) test is
\begin{equation*}
  R \|\delta_m \| < 0.1 \epsilon \, .
\end{equation*}
We allow at most 3 iterations (but this limit can be changed by the
user).  We also declare the iteration to be diverging if any
$\|\delta_m\|/\|\delta_{m-1}\| > 2$ with $m > 1$. If the iteration
fails to converge with a current $J$ or $P$, we are forced to reduce
the step size, and we replace $h_n$ by $h_n/4$.  The integration is
halted after a preset number of convergence failures; the default
value of this limit is 10, but this can be changed by the user.

When SPGMR is used to solve the linear system, its errors must also be
controlled, and this also involves the local error test constant
$\epsilon$.  The linear iteration error in the solution vector
$\delta_m$ is approximated by the preconditioned residual vector.
Thus to ensure (or attempt to ensure) that the linear iteration errors
do not interfere with the nonlinear error and local integration error
controls, we require that the norm of the preconditioned residual in
SPGMR is less than $0.05 \cdot (0.1 \epsilon)$.

% Jacobian DQ approximations
%---------------------------
With the direct dense and band methods, the Jacobian may be supplied
by a user routine, or approximated by difference quotients,
at the user's option.  In the latter case, we use the usual
approximation
\[ J_{ij} = [f^i(t,y+\sigma_j e_j) - f^i(t,y)]/\sigma_j \, . \]
The increments $\sigma_j$ are given by
\[ \sigma_j = \max\left\{\sqrt{U} \; |y^j| , \sigma_0 W_j \right\} \, , \]
where $U$ is the unit roundoff, $\sigma_0$ is a dimensionless value
(involving the unit roundoff and the norm of $\dot{y}$),
and $W_j$ is the error weight defined in (\ref{errwt}).  In the dense
case, this scheme requires $N$ evaluations of $f$, one for each column
of $J$.  In the band case, the columns of $J$ are computed in groups,
by the Curtis-Powell-Reid algorithm \cite{CPR:74}, with the number of
$f$ evaluations equal to the bandwidth.

In the case of SPGMR, preconditioning may be used on the left, on the
right, or both, with user-supplied routines for the preconditioning
setup and solve operations, and optionally also for the required
matrix-vector products $Jv$.  If a routine for $Jv$ is not supplied,
these products are computed as
\begin{equation}\label{jacobv}
Jv = [f(t,y+\sigma v) - f(t,y)]/\sigma \, .
\end{equation}
The increment $\sigma$ is $1/\|v\|$, so that $\sigma v$ has norm 1.

% Error test
%-----------
A critical part of CVODE, making it an ODE ``solver'' rather than
just an ODE method, is its control of local error.  At every step, the
local error is estimated and required to satisfy tolerance conditions,
and the step is redone with reduced step size whenever that error test
fails.  As with any linear multistep method, the local truncation
error LTE, at order $q$ and step size $h$, satisfies an asymptotic
relation
\[ \mbox{LTE} = C h^{q+1} y^{(q+1)} + O(h^{q+2}) \]
for some constant $C$, under mild assumptions on the step sizes.
A similar relation holds for the error in the predictor $y_{n,0}$.
These are combined to get a relation
\[ \mbox{LTE} = C' [y_n - y_{n,0}] + O(h^{q+2}) \, , \]
where $C'$ is another known constant.  The local error test is simply
$\|\mbox{LTE}\| \leq 1$ (recalling that a vector of WRMS norm 1 is
considered small).  Using $y_n = y_{n,m}$ (the last iterate computed)
above, the local error test is performed on the predictor-corrector
difference $\Delta_n \equiv y_{n,m} - y_{n,0}$, and takes the form
\[ \|\Delta_n\| \leq \epsilon \equiv 1/|C'| \, . \]
If this test passes, the step is considered successful.  If it fails,
the step is rejected and a new step size $h'$ is computed based on the
asymptotic behavior of the local error, namely by the equation
\[ (h'/h)^{q+1} \|\Delta_n\| = \epsilon/6 \, . \]
Here 1/6 is a safety factor.  A new attempt at the step is made,
and the error test repeated.  If it fails three times, then the order
$q$ is reset to 1 (if it was $> 1$), or (if $q = 1$) the step is
restarted from a fresh value of $f$ (discarding all history data).
The ratio $h'/h$ is restricted (during the current step only) to be
$\leq 0.2$ after two error test failures, and to be $\geq 0.1$ after
three.  After seven failures, CVODE returns to the user with a give-up
message.

% Step/order control
%-------------------
In addition to adjusting the step size to meet the local error test,
CVODE periodically adjusts the order, with the goal of maximizing the
step size.  The integration starts out at order 1 and varies the order
dynamically after that.  The basic idea is to pick the order $q$ for
which a polynomial of order $q$ best fits the discrete data involved
in the multistep method.  However, if either a convergence failure or
an error test failure occurs on any given step, no change in step
size or order is allowed on the next step.  At the current order $q$,
selecting a new step size is done exactly as when the error test
fails, giving a tentative step size ratio
\[ h'/h = (\epsilon / 6 \|\Delta_n\| )^{1/(q+1)} \equiv \eta_q \, . \]
We consider changing order only after taking $q+1$ steps at order $q$,
and then we consider only orders $q' = q - 1$ (if $q > 1$) or
$q' = q + 1$ (if $q < $ max. order allowed).  The local truncation
error at order $q'$ is estimated using the history data.  Then a
tentative step size ratio is computed on the basis that this error,
LTE$_{q'}$, behaves asymptotically as $h^{q'+1}$.  With safety factors
of 1/6 and 1/10 respectively, these ratios are:
\[ h'/h = [1 / 6 \|\mbox{LTE}_{q-1}\| ]^{1/q} \equiv \eta_{q-1} \]
and
\[ h'/h = [1 / 10 \|\mbox{LTE}_{q+1}\| ]^{1/(q+2)} \equiv \eta_{q+1} \, . \]
The new order and step size are then set according to
\[ \eta = \max\{\eta_{q-1},\eta_q,\eta_{q+1}\} ~,~~ h' = \eta h \, , \]
with $q'$ set to the index achieving the above maximum.
However, if we find that $\eta < 1.5$, we do not bother with the
change.  Also, $h'/h$ is always limited to 10, except on the first
step, when it is limited to $10^4$.

The various algorithmic features of CVODE described above, as
inherited from VODE and VODPK, are documented in
\cite{BBH:89,Byr:92,Hin:00}.  A full description of the usage
of CVODE is given in \cite{HiSe:04cvode}.

% Stability limit detection
%--------------------------
There is an important additional part of the CVODE order selection
algorithm that is not based on local error, but instead provides
protection against potentially unstable behavior of the BDF
methods. At order 1 or 2, the BDF method is A-stable.  But at
orders 3 to 5 it is not, and the region of instability includes a
portion of the left half-plane that is concentrated near the
imaginary axis.  The size of that region of instability grows as
the order increases from 3 to 5. What this means is that when
running BDF at these higher orders, if an eigenvalue $\lambda$ of
the system lies close enough to the imaginary axis, the step
sizes, $h$, for which the method is stable are limited (at least
according to the linear stability theory) to a set that prevents
$h\lambda$ from leaving the stability region.  System eigenvalues
that are likely to cause this instability are ones that correspond
to weakly damped oscillations, such as might arise from a
semi-discretized advection-diffusion PDE with advection dominating
over diffusion.

CVODE includes an optional algorithm called STALD (STAbility Limit
Detection), which attempts to detect directly the presence of a
stability region boundary that is limiting the step sizes in the
presence of a weakly damped oscillation \cite{Hin:92}.  Working
directly with history data that is readily available, if it concludes
that the step size is in fact stability-limited, it dictates a
reduction in the method order, regardless of the outcome of the
error-based algorithm.

STALD has been tested in combination with the VODE solver on linear
advection-dominated advection-diffusion problems \cite{Hin:95}, where
it works well.  The implementation in CVODE has been successfully
tested on linear and nonlinear advection-diffusion problems, among others.
The STALD option adds some overhead computational cost to the CVODE
solution.  In timing tests, these overhead costs have ranged from 2\%
to 7\% of the total, depending on the size and complexity of the
problem, with lower relative costs for larger problems.  Therefore, it
should be activated only when there is reasonable expectation of modes
in the user's system for which it is appropriate, together with poor
performance at orders 3--5, for no apparent reason, with the option
turned off.

% Output modes
%-------------
Normally, CVODE takes steps until a user-defined output value
$t = t_{\mbox{\scriptsize out}}$ is overtaken, and then it computes
$y(t_{\mbox{\scriptsize out}})$ by interpolation.  However, a
``one step'' mode option is available, where control returns to the
calling program after each step.  There are also options to force
CVODE not to integrate past a given stopping point
$t = t_{\mbox{\scriptsize stop}}$.

% Rootfinding
%------------
Lastly, CVODE has been augmented to include a rootfinding feature,
whereby the roots of a set of user-defined functions $g_i(t,y)$
can be found while integrating the initial value problem for
$y(t)$.  The algorithm checks for changes in sign in the
$g_i$ over each time step, and when a sign change is found, it
homes in on the root(s) with a weighted secant iteration method
\cite{HeSh:80}. (CVODE also checks for exact zeros of the $g_i$.)
The iteration stops when the root is bracketed within a tolerance
that is near the roundoff level of $t$.
