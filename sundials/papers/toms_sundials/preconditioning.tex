\section{Preconditioning}\label{s:preconditioning}

All of the SUNDIALS solvers make repeated use of a Krylov method to
solve linear systems of the form $A$ (correction vector) =
-(residual vector), where $A$ is an appropriate Jacobian or Newton
matrix.  But simple (unpreconditioned) Krylov methods are rarely
successful; it is generally necessary to precondition the system
in order to obtain acceptable efficiency.  A system $A x = b$ can be
preconditioned on the left, as $(P^{-1}A) x = P^{-1} b$;
on the right, as $(A P^{-1}) P x = b$; or on both sides, as
$(P_L^{-1} A P_R^{-1}) P_R x = P_L^{-1}b$.  The Krylov method is then
applied to a system with the matrix $P^{-1}A$, or $AP^{-1}$, or
$P_L^{-1} A P_R^{-1}$, instead of $A$.  In order to improve the
convergence of the Krylov iteration, the preconditioner matrix $P$, or
the product $P_L P_R$ in the last case, should in some sense
approximate the system matrix $A$.  Yet at the same time, in order to
be cost-effective, the matrix $P$, or matrices $P_L$ and $P_R$, should
be reasonably efficient to evaluate and solve.  Finding a good point
in this tradeoff between rapid convergence and low cost can be very
difficult.  Good choices are often problem-dependent, but not always,
as we show below.

The CVODE and CVODES solvers allow for preconditioning either
side, or on both sides, although we know of no situation where
preconditioning on both sides is clearly superior to
preconditioning on one side only (with the product $P_L P_R$).  In
contrast, as noted in the previous section, KINSOL allows only
right preconditioning, while IDA and IDAS allow only left preconditioning.

Typical preconditioners used with the solvers in SUNDIALS are based on
approximations to the Jacobian matrices of the systems involved.
Because the Krylov iteration occurs within a Newton iteration, and
often also within a time integration, and each of these iterations has
its own test for convergence, the preconditioner may use a very crude
approximation, as long as it captures the dominant numerical
feature(s) of the system.  We have found that the combination of a
preconditioner with the Newton-Krylov iteration, using even a fairly
poor approximation to the Jacobian, can be surprisingly superior to
using the same matrix without Krylov acceleration (i.e., a modified
Newton iteration), as well as to using the Newton-Krylov method with
no preconditioning.

We further exploit this nested iteration setting, and differences in
the costs of the various preconditioner operations, by treating in two
separate phases each preconditioner matrix $P$ involved:
\begin{itemize}
\item a setup phase: evaluate and preprocess $P$ (done infrequently), and
\item a solve phase: solve systems $Px = b$ (done frequently).
\end{itemize}
Accordingly, the user of each solver must supply two separate
routines for these operations.  The setup of $P$ is generally more
expensive than the solve operation, and so it is done as infrequently
as possible, with updates to $P$ dictated primarily by convergence
failures of the Newton iteration.  The system solves $Px = b$ must of
course be done at every Krylov iteration (once for each matrix in
the case of two-sided preconditioning).

We provide help to SUNDIALS users with respect to preconditioning in
two ways.  First, for each solver, there is at least one example
problem program which illustrates a preconditioner for
reaction-diffusion systems, based on the concept of operator
splitting.  The example does not perform operator splitting (which
generally requires giving up error control), but builds the
preconditioner from one of two operators (reaction) in the problem.
These examples are intended to serve as templates for possible
user-defined preconditioners in similar applications.  See
\cite{BrHi:89} for an extensive study of preconditioners for
reaction-transport systems.

Second, the SUNDIALS package includes some extra preconditioner
modules, for optional use with the solvers.  For parallel environments,
each of the SUNDIALS solvers provides a preconditioner module which
generates a band-block-diagonal (BBD) preconditioner.  For serial
environments, CVODE and CVODES also supply a band preconditioner module.
These band and BBD preconditioners are described below.  Full details on the
usage of these optional modules are given in the respective user
guides --- \cite{HiSe:04cvode,HiSe:04cvodes,HSW:04kinsol,HiSe:04ida}.

In any case, for any given choice of the approximate Jacobian, it may
be best to consider choices for the preconditioner linear solver that
are more appropriate to the specific problem than those supplied with
SUNDIALS.

\subsection{Preconditioners for CVODE}

Assuming that the CVODE user has chosen one of the stiff system
options, recall from (\ref{Newtonmat}) that the Newton matrix for
the nonlinear iteration has the form $I - \gamma J$, where $J$ is the
ODE system Jacobian $J = \partial f / \partial y$.  Therefore, a
typical choice for the preconditioner matrix $P$ is
\begin{equation*}
  P = I - \gamma \tilde{J} \, , \mbox{ with } \tilde{J} \approx J \, .
\end{equation*}
As noted above, the approximation may be a crude one.

The setup phase for $P$ is generally performed only once every several
time steps, in an attempt to minimize costs.  In addition to
evaluating $P$, it may involve preprocessing operations, such as LU
decomposition, suitable for later use in the solve phase.  Within the
setup routine, the user can save and reuse the relevant parts of the
approximate Jacobian $\tilde{J}$, as directed by CVODE (in its call to
the user routine), so as to further reduce costs when the scalar
$\gamma$ has changed since the last setup call.  This option requires the
user to manage the storage of the saved data involved.  But this
tradeoff of storage for potential savings in computation may be
beneficial if the cost of evaluating $\tilde{J}$ is significant in
comparison with the other operations performed on $P$.

For serial environments, CVODE supplies a preconditioner called
CVBANDPRE, whose use is optional.  This preconditioner computes
and solves a banded approximation $P$ to the Newton matrix,
computed with difference quotient approximations.  The user
supplies a pair of lower and upper half-bandwidths --- {\tt ml,
mu} --- that define the shape of the approximate Jacobian
$\tilde{J}$; its full bandwidth is {\tt ml+mu+1}.  $\tilde{J}$ is
computed using difference quotients, with {\tt ml+mu+1}
evaluations of $f$.  The true Jacobian need not be banded, or its
true bandwidth may be larger, as long as $\tilde{J}$ approximates
$J$ sufficiently well.

Extending this idea to the parallel setting, CVODE also includes a
module, called CVBBDPRE, that generates a band-block-diagonal
preconditioner.  CVBBDPRE is designed for PDE-based problems and uses the
idea of domain decomposition, as follows.  Suppose that a
time-dependent PDE system, with the spatial operators suitably
discretized, yields the ODE system $\dot{y} = f(t,y)$.  Now consider a
decomposition of the (discretized) spatial domain into $M$
non-overlapping subdomains.  This decomposition induces a block form
$y = (y_1,\ldots,y_M)$ for the vector $y$, and similarly for $f$.  We
will use this distribution for the solution with CVODE on $M$
processors.

The $m$-th block of $f$, $f_m(t,y)$, depends on both $y_m$ and ghost
cell data from other blocks $y_{m'}$, typically in a local manner,
according to the discretized spatial operators.  However, when we
build the preconditioner $P$, we will ignore that coupling and
include only the diagonal blocks $\partial f_m / \partial y_m$.  In
addition, it may be cost-effective to exclude from $P$ some parts of
the function $f$.  Thus, for the computation of these blocks, we
replace $f$ by a function $g \approx f$ (and $g = f$ is certainly
allowed).  For example, $g$ may be chosen to have a smaller set of ghost
cell data than $f$.  In the CVBBDPRE module, the matrix blocks
$\partial g_m/\partial y_m$ are approximated by band matrices $J_m$,
again exploiting the local spatial coupling, and on processor $m$
these matrices
are computed by a difference quotient scheme.  Then the complete
preconditioner is given by
\begin{equation*}
  P = \mbox{diag}[P_1,\ldots,P_M] \, , \quad
  P_m = I_m - \gamma J_m \, .
\end{equation*}
Linear systems $Px = b$ are then solved by banded LU and backsolve
operations on each processor.  The setup phase consists of the
evaluation and banded LU decomposition of $P_m$, and the solve phase
consists of a banded backsolve operation.

In order to minimize costs in the difference quotient scheme, the
function $g$ is supplied by the user in the form of two routines.  One
routine, called once per $P$ evaluation, performs inter-processor
communication of data needed to evaluate the $g_m$.  The other routine
evaluates $g_m$ on processor $m$, assuming that the communication
routine has already been called.  The banded structure of the problem
is exploited in two different ways.  First, the user supplies a pair of
half-bandwidths, {\tt ml} and {\tt mu}, that defines the shape of the
matrix $J_m$.  But the user also supplies a second pair of
half-bandwidths, {\tt mldq} and {\tt mudq}, for use in the difference
quotient scheme, in which $J_m$ is computed by way of
{\tt mldq+mudq+2} evaluations of $g_m$.  The values {\tt ml} and {\tt mu}
may be smaller than {\tt mldq} and {\tt mudq}, giving a tradeoff between
lower matrix costs and slower convergence.  Thus, for example, a matrix
based on 5-point coupling in
2D ({\tt mldq = mudq = } mesh dimension) might be well approximated by
a tridiagonal matrix ({\tt ml = mu = 1}).  In any case, for the sake
of efficiency, both pairs of half-bandwidths may be less than the true
values for $\partial g_m /\partial y_m$, and both pairs may depend on
$m$.

\subsection{Preconditioners for KINSOL and IDA}

The KINSOL package includes a module, called KINBBDPRE, that provides
a band-block-diagonal preconditioner for use in parallel environments,
analogous to that of the CVODE module CVBBDPRE.  Here the problem to
be solved is $F(u) = 0$, and the preconditioner is constructed by way
of a function $g \approx F$.  Namely, it is defined as
\begin{equation*}
  P = \mbox{diag}[P_1,\ldots,P_M] \, , \quad
  P_m \approx \partial g_m / \partial u_m \, ,
\end{equation*}
in terms of the blocks of $g$ and $u$ on processor $m$.  Again, $P_m$
is banded and is computed using difference quotients, with user-supplied
half-bandwidths for both the difference quotient scheme and the
retained band matrix.

Likewise, the IDA package, in the parallel setting, includes a
band-block-diagonal preconditioner module, called IDABBDPRE.  For the
problem $F(t,y,{\dot y}) = 0$, the preconditioner is defined by way of a
function $G \approx F$.  Specifically, the preconditioner is
\begin{equation*}
  P = \mbox{diag}[P_1,\ldots,P_M] \, ,
  \quad P_m \approx \partial G_m / \partial y_m
  + \alpha \partial G_m / \partial {\dot y}_m \, .
\end{equation*}
Each block $P_m$ is banded, computed using difference quotients, with
user-supplied half-bandwidths for the difference quotient scheme and
the retained matrix.
