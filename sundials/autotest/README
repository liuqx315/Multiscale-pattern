--------------------------------------------------------------------------------

                                      SATS
                      (SUNDIALS Automated Testing Software)

--------------------------------------------------------------------------------

*********************************
* Section 1: Brief Introduction *
*********************************

The SATS suite was developed to help automate the task of testing SUNDIALS and
consists of the following shell scripts and text file:

   sundials.sh - main SATS script which coordinates actions of other scripts
                 (invoked by user)

   sundials_build.sh - builds SUNDIALS

   sundials_test.sh - runs SUNDIALS examples

   sundials_check_bash.sh - modifies BASH shell interpretor path if necessary

   sundials_timeout.sh - performs post-test cleanup procedure

   sundials_errors.txt - contains error keywords used when searching test
                          logs for error messages

******************************
* Section 2: Configuring SSH *
******************************

If the SATS suite will be used to test SUNDIALS on a remote system or if
SUNDIALS will use MPICH-MPI on the local system, then the local SSH daemon
must be configured to allow public-key authentication so the user can log onto
a system without being prompted to enter a password.  Since LC systems are
generally setup to permit use of a public-key-based authentication scheme, then
all that remains to be done is to properly initialize the local SSH environment.

   Configuring OpenSSH client:

   Step 1: Generate an RSA-based public/private key pair for SSH-1 (protocol
           version 1).  When prompted to enter a passphrase press [RETURN] to
           use a blank passphrase.

              # ssh-keygen -f ~/.ssh/identity -t rsa1

   Step 2: Add the public key just generated (~/.ssh/identity.pub) to the
           authorized_keys file (~/.ssh/authorized_keys) on the local system.
           Now any CASC Linux or Solaris system may be logged onto via SSH
           (using SSH-1) without entering a password.

              # cat ~/.ssh/identity.pub >> ~/.ssh/authorized_keys

   Step 3: Connect to an OCF system and add the public authentication key
           (localhost:~/.ssh/identity.pub) to the authorized_keys file on the
           remote system (remote_system:~/.ssh/authorized_keys).  Each entry in
           the authorized_keys file should only be a single line and spacing
           between fields is significant, so be careful if using a cut-and-paste
           facility.  Now any OCF system may be logged onto via SSH
           (using SSH-1) without being prompted for a password.

   Step 4: To initialize the known_hosts file, connect to each remote system
           to be tested and answer "yes" when prompted, but use the command
           "ssh -1 system_name" instead of "ssh system_name" since only
           SSH-1 has been configured thus far in the procedure.  Each system
           will now have an entry in the known_hosts file
           (~/.ssh/known_hosts).

           Note: Since the GPS cluster utilizes a node pool it will be necessary
                 to connect to nodes gps15-22.

   Step 5: Generate an RSA-based public/private key pair for SSH-2.  When
           prompted to enter a passphrase press [RETURN] to use a blank
           passphrase.

              # ssh-keygen -f ~/.ssh/id_rsa -t rsa

   Step 6: Add the public key just generated (~/.ssh/id_rsa.pub) to the
           authorized_keys file (~/.ssh/authorized_keys) on the local system.

              # cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

   Step 7: Connect to an OCF system and add the public authentication key
           (localhost:~/.ssh/id_rsa.pub) to the authorized_keys file on the
           remote system (remote_system:~/.ssh/authorized_keys).

   Step 8: Connect to each remote system and answer "yes" when prompted, but use
           the command "ssh system_name" instead of "ssh -1 system_name" since
           SSH-1 has already been configured.

           Note: If using the "localhost" keyword to refer to the local system
                 in a custom MPICH-MPI machine/host file, then it will also be
                 necessary to connect to "localhost".

**********************************
* Section 3: Configuration Files *
**********************************

The SATS suite only requires one configuration file, but two additional types
of MPI-related configuration files may also be used.  The main configuration
file defines several SATS-specific environment variables and includes system
configuration directives.  The two optional configuration files are
system-specific and may be used to help define the MPI environment.

The following environment variables are defined within the required
configuration file:

   PROJECT_NAME=project_name          Default: none (must be defined)
      * name assigned to the project and also the name of the top-level
        directory of the source tree

   LOCAL_USERNAME=username          Default: current user
      * username of user invoking the main SATS script

   LOCAL_MACHINE=system_name          Default: local system
      * host name of the local system without the domain name

   LOCAL_DIR=directory          Default: none (must be defined)
      * local work/scratch directory where SATS-related temporary files
        may be kept

   REMOTE_USERNAME=username          Default: LOCAL_USERNAME
      * username to use when logging onto remote systems

   REMOTE_MACHINES="system1 system2..."          Default: LOCAL_MACHINE
      * list of all systems to be tested

   MAIL_RECIPIENTS="user1 user2..."          Default: LOCAL_USERNAME@llnl.gov
      * list of usernames and/or e-mail addresses to which error messages
        should be sent
      * if the e-mail message cannot be sent, then the error message is
        renamed (check script output for filename) and stored under the
        directory LOG_DIR/archive

   AUTOTEST_ADMIN=username          Default: LOCAL_USERNAME
      * not currently used by SATS but may be used by a wrapper script to
        determine to whom script output should be e-mailed when SATS is run
        as a background task

   MAX_TIME=num_minutes          Default: none (must be defined)
      * maximum allowable running time for each process

   LOG_DIR=directory          Default: LOCAL_DIR/log
      * local directory where all SATS-related log files may be kept

   SCRIPT_DIR=directory          Default: current working directory
      * local directory containing all five SATS scripts and the
        sundials_errors.txt file

   MPI_HOST_FILE_DIR=directory          Default: none (optional)
      * local directory where all MPICH-MPI-style host files
        (filename: system_name.host)may be found

   MPI_OPT_DIR=directory          Default: directory containing main input file
      * local directory where all optional MPI-related configuration files
        (filename: system_name.opt) may be found

   SOURCE_DIR=directory          Default: none (use CVS) (optional)
      * top-level directory of local copy of project source tree to be used

   NUM_LOG_FILES=num_files          Default: 1
      * number of SATS log files to keep

The main configuration file must also include system configuration directives
of the following format:

   #system_name  remote_dir  config_opts

      #system_name - host name/cluster name without domain name (should match
                     entry in REMOTE_MACHINES) preceded by the "#" symbol

      remote_dir - either valid directory on system or "default_dir" keyword
                   which specifies that remote_dir=LOCAL_DIR

      config_opts - configure options to be passed to SUNDIALS configure script

The first type of optional input file (filename: system_name.opt) specifies
which MPI installation is to be used by defining the following three environment
variables:

   MPI_VERSION=mpich/lam/default          Default: default (use system default)
      * MPI implementation to use - "default" keyword means system default
        should be used (determined by sundials_test.sh script)

   MPI_DIR=directory          Default: none (optional)
      * base installation directory of MPI implementation to use
      * should only be defined if MPI_VERSION != default

   MPI_COMMAND="mpirun_command"          Default: default (use system default)
      * command to use when running all parallel SUNDIALS examples - "default"
        keyword means system default should be used (defined in
        sundials_test.sh script)

Note: SATS will automatically generate a default *.opt file for a system
      if none exists.

The second type of optional input file (filename: system_name.host) is only
significant for systems using MPICH-MPI and is simply an MPICH-MPI-style host
file.  The host file is used by the sundials_timeout.sh script to cleanup IPC
resources and kill any unresponsive processes on remote systems.

Note: Sample configuration files are provided - sample.input, sample.opt and
      sample.host.

***************************
* Section 4: Running SATS *
***************************

After modifying/creating all of the necessary configuration files, then SATS
may be invoked by issuing the following command from a shell prompt:

   # ./sundials.sh main_config_file

Only the main configuration file needs to be supplied as an argument to the main
SATS script since the optional configuration files will be automatically
referenced if needed.

The output generated by the sundials.sh script includes useful information and
may be used to gauge the overall progress of the testing procedure.

********************************
* Section 5: Viewing Log Files *
********************************

Upon completion of the testing procedure, a master archive file (filename:
sundials-logs.tar-num, where 0 <= num <= 9) containing all of the retrieved log
files is stored under the directory LOG_DIR/archive.  Each archive file
contains the build logs (filenames: sundials-build-logs.tar.gz and
sundials-build-logs-ERROR.tar.gz) and test logs (filenames:
sundials-test-logs.tar.gz and sundials-test-logs-ERROR.tar.gz) from each system,
and a copy of the error message (filename: sundials.info).

**********************************
* Section 6: Compatibility Notes *
**********************************

The current revision of the SATS suite should only be invoked on a
Linux/Solaris system and may be used to test SUNDIALS on the following
LC and CASC systems:
   * Blue/Frost (AIX Unix)
   * GPS (Tru64 Unix)
   * CASC SUN Cluster (Solaris)
   * CASC Linux Cluster (Red Hat Linux)

Although the above list should be sufficient to test portability, adding
support for additional systems may be done by making appropriate
modifications to sundials_test.sh and sundials_timeout.sh.

--------------------------------------------------------------------------------

Note: Please send comments or suggestions to Aaron Collier (collier6@llnl.gov).

--------------------------------------------------------------------------------

$Revision: 1.1 $
$Date: 2004-04-02 21:18:20 $
