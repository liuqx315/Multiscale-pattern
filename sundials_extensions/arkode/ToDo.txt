Remaining components to build:


Easy:
[X] Add an ark_mem integer member denoting the current stage
[X] ARK nonlinear residual function.
[X] Code to increment ark_nst_con counter when we experience a
    convergence failure and need to take a smaller step.  The
    code/logic may already be there in using ark_ncfn.  However,
    consider whether this counter should differ at all from
    ark_nst_con (I can't think of any difference at the moment).
[X] Expand statistics to count, store and return fe and fi calls
    separately.
[X] Dense output evaluation routine, both for solution value and
    derivatives.  From Hairer, Norsett & Wanner (pp 190-191), we can
    build dense output of accuracy O(h^3) using the data {yn,yp,fn,fp}
    via the Hermite cubic interpolant, 
       y(tau) = (1-tau)*yn + tau*yp 
              + tau*(tau-1)*((1-2*tau)(yp-yn) + (tau-1)*h*fn + tau*h*fp)
    where 0\le tau\le 1 transitions us between tn and tp.  This is
    O(h^3) for y, but only O(h^2) for y'.  If higher order accuracy is
    desired, we may increase the order of the polynomial by evaluating
    f at intermediate points, with argument as the interpolated
    version of the function at the current order.  Each stage of this
    will increase the order by 1, and all intermediate stages must be
    chosen to be independent of one another.
[X] Convert dense output order to a Set routine, so that default is 
    always 3 unless otherwise specified.  Moreover, if it IS specified 
    to be higher than 3, we could then allocate storage for fa and fb
    in memory, along with a flag denoting whether these were current,
    and if they are current then we don't need to evaluate them.
[X] Predictor routine to store guesses in Fi array (since
    that is only filled in after setting the initial guess).  This
    will be called once at the end of a successful step to generate
    predictions for the next one.
[X] Routine to calculate time step solution and embedding, along with 
    time accuracy error estimate.
[X] Code in InitialSetup routine to fill Fi array with initial
    condition (trivial predictor) for stages in first step.
[X] Routine to select appropriate Butcher table for the problem, given 
    the relevant user-defined constraints.
[X] Get routine to output the Butcher tables.
[X] Set routine to manually choose a pre-existing Butcher table (by
    number?, by name?) -- i.e. not user-supplied, and not the default
    choice.
[ ] Code to fill in values for last successful step: told, hold, yold,
    ynew, fold, fnew.  For arrays, use pointer swap to omit need to
    copy values from ynew to yold or fnew to fold.  Still do the copy
    to fill ynew and fnew, since user may input y itself into code,
    and pointer should remain fixed.
[ ] Routine to do actual step prediction [put inside existing
    ARKPredict]: this will copy the value from Fi into the nonlinear
    solver initial guess (for early stages), or it can use
    already-generated stage solutions to predict subsequent stages
    (for later stages).  This will be called prior to every stage
    solve.
[ ] Code to clean/check Butcher tables -- this should check that ERK
    and IRK tables share root & canopy nodes, that all necessary
    coefficients are defined.  This should be called inside
    ARKInitialSetup.  DO I WANT TO STORE DIFFERENT ROOT/CANOPY NODES
    FOR EACH METHOD AT FIRST, AND THEN CHECK THESE TO BE CERTAIN
    THEY'RE LEGAL?
[ ] In all set routines, update code to allow specification of the
    defaults through supplying zeros/NULL to the function.
[ ] stageprep routine, to evaluate Fe on current solution (unless I
    can include this stuff in something that already exists)
[ ] Initialize and update ark_mem->ark_hold within time-stepping.  Or
    maybe it is already stored in ark_hu??
[ ] Ensure that after calling ARKodeReInit (and ark_nst is reset to
    0), that none of the stuff I put into ARKInitialSetup will
    backfire if called again.
[ ] If I need to use any currently-unused N_Vector operations, add
    them to ARKCheckNvector routine.
[ ] Consider removing integer parameter ark_smax_alloc, since I can
    always check whether the N_Vector is NULL or not and
    allocate/deallocate appropriately.
[ ] Remove ark_bd data arrays since we're using a different form of
    dense output.
[ ] Update the total real/integer work length estimates in
    ARKodeCreate. 


Medium:
[ ] Build a new linear solver to directly tackle the stage solves;
    call this when the user has specified that the system is linear
    (so J only needs to be set up once).  Consider calling this to
    handle calculation of the updated solution and embedding when 
    M != I (of course, this would mean that I only allow linear
    operators for M, but that's a reasonable restriction).
[ ] Update ARKStep loop to do stage-specific solves.  Add
    code to always check the diagonal value inside Ai.  If it's zero
    (ERK or ESDIRK) I just declare success (unless M != I).  If I just
    declare success, I should be certain to still evaluate Fi at the
    "solution" of the stage, since it would occur already within an
    implicit solve, and then I can just trust that it is still
    available for subsequent stages.  This is also where I could have
    logic to decide between calling Newton or just a linear solver for
    the stage solution.
[ ] User-defined operator M : R^n -> R^n, and logic to account for
    this in step solution & embedding.  Update time step
    solution/embedding routine to now do solves to compute these.
[ ] Code to allow a change in the Butcher table, both to allow an
    increased/decreased number of stages (no real issue for decrease,
    don't even need to free the N_Vectors if I don't want to; need to
    malloc the new vectors for an increase, though), and to do the
    relevant checking/cleaning of the Butcher table, etc..  This could
    occur only inside ARKodeReInit, since that's the moment where I
    would allow this kind of thing (and would need to reset accuracy
    history data). 
[ ] Check all linear solvers to ensure that they setup/solve the
    correct system, and access all of the correct data out of ark_mem.
[ ] Insert calls to my ARKAdapt function wherever CVODE currently
    decides on accuracy- or stability-based time step adaptivity.
[ ] Think about scenario in which ARK method's first stage doesn't set
    c[0] = 0.0:  I need to fill in fold and fnew for dense output and
    predictors, so if they are already stored in Fe[0],Fi[0] or
    Fe[1],Fi[1] I could just copy them; otherwise I'll need to
    compute them at the end of each successful step.
[ ] Think about scenario where an attempted step has too much error
    (but still converges within iteration limits).  Add code to
    ARKRestore() to recompute predictions for implicit solutions at
    these shorter times. 
[ ] Go through new and existing routines to determine whether we need
    additional #define control constants, return values and error
    messages; also prune out unused ones.
[ ] Build a Matlab routine to check the analytical order of accuracy
    and B-stability of a generic Butcher table.  Could even have a
    routine that would evaluate the stability function for a variety
    of h*lambda values to test for A-stability (and even L-stability).
[ ] Go through logic of when everything is updated, especially wrt the
    predictor data (and if it's ever called after a failed step).



Hard:
[ ] Enable my solver and disable BDF solver.  This will require
    searching through the code for all of the marked locations where I
    made notes to myself (in all caps, and usually preceded by /*****)
[ ] Remove BDF code.
[ ] Look into root-finding stuff.
[ ] Move all fancy logic determination stuff that's currently located
    in specific solve routines (including linear solvers) to reside in
    a self-contained set of functions that ARKode provides.  Attach
    these functions to linear solver data structures where relevant.
[ ] Form corresponding nonlinear solver data structures that will
    contain function pointers for logic arguments, nonlinear residual
    calculations, tolerances, etc., and have Newton method use
    information from that structure instead of calling items that are
    hard-coded into ark_mem.


Unknown/Questions:
[ ] Code for customization of time accuracy norm?  Since SUNDIALS does 
    not implement a general p-norm, I could swap between WRMS and WMax
    norms, though the Wmax norm will require a bit of work (product
    followed by max).  This might break all of our other heuristics,
    especially since the nonlinear/linear solvers all use WRMS, so
    maybe I should NOT do this?
[ ] Code to select the best time step estimation method & parameters
    for a specific method?  I don't actually know what this would be
    for ERK, DIRK, ESDIRK, ARK, etc., but if I did then I should pick
    the best.  Maybe I should build the functionality first, but leave
    it disabled until after I can run exhaustive testing of the full
    suite of implemented methods on a range of test problems?
[ ] Code to perform automated explicit time step stability estimates.
    This could be approximate, using a "random" vector and
    approximating the Jacobian of Fe via finite differencing, and then
    computing the Ritz quotient to estimate the largest eigenvalue.
    Of course, since each ERK has a different stability region, maybe
    some of that information should be included as well?  Also, if Fe
    has complex eigenvalues, the Ritz quotient cannot output complex
    values since it is the ratio of 2 real numbers.  What could I do
    in that scenario?  Need to read up on this kind of thing: has it
    been done before, what did they do, can I do it with the tools at
    my disposal? 
[ ] Consider changing logic for initial timestep estimate, since I
    do not need to start the calculation with a 1st order method and
    bootstrap my way up.
[ ] Consider whether a non-identity matrix M will change how
    preconditioning occurs, i.e. whether it disables either right or
    left preconditioning.
[ ] Consider all the places where I use Fe[] and Fi[] for temporary
    data.  If I'm running a purely implicit or explicit calculation,
    then the other set of these is never allocated!!  Figure out how
    to deal with this situation!
