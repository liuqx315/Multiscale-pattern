Remaining components to build:

[X] Add an ark_mem integer member denoting the current stage
[X] ARK nonlinear residual function.
[X] Code to increment ark_nst_con counter when we experience a
    convergence failure and need to take a smaller step.  The
    code/logic may already be there in using ark_ncfn.  However,
    consider whether this counter should differ at all from
    ark_nst_con (I can't think of any difference at the moment).
[X] Expand statistics to count, store and return fe and fi calls
    separately.
[X] Dense output evaluation routine, both for solution value and
    derivatives.  From Hairer, Norsett & Wanner (pp 190-191), we can
    build dense output of accuracy O(h^3) using the data {yn,yp,fn,fp}
    via the Hermite cubic interpolant, 
       y(tau) = (1-tau)*yn + tau*yp 
              + tau*(tau-1)*((1-2*tau)(yp-yn) + (tau-1)*h*fn + tau*h*fp)
    where 0\le tau\le 1 transitions us between tn and tp.  This is
    O(h^3) for y, but only O(h^2) for y'.  If higher order accuracy is
    desired, we may increase the order of the polynomial by evaluating
    f at intermediate points, with argument as the interpolated
    version of the function at the current order.  Each stage of this
    will increase the order by 1, and all intermediate stages must be
    chosen to be independent of one another.
[X] Convert dense output order to a Set routine, so that default is 
    always 3 unless otherwise specified.  Moreover, if it IS specified 
    to be higher than 3, we could then allocate storage for fa and fb
    in memory, along with a flag denoting whether these were current,
    and if they are current then we don't need to evaluate them.
[X] Predictor routine to store guesses in Fi array (since
    that is only filled in after setting the initial guess).
[X] Routine to calculate time step solution and embedding, along with 
    time accuracy error estimate.
[X] Code in InitialSetup routine to fill Fi array with initial
    condition (trivial predictor) for stages in first step.
[X] Routine to select appropriate Butcher table for the problem, given 
    the relevant user-defined constraints.
[X] Get routine to output the Butcher tables.
[X] Set routine to manually choose a pre-existing Butcher table (by
    number?, by name?) -- i.e. not user-supplied, and not the default
    choice.
[X] In all set routines, update code to allow specification of the
    defaults through supplying zeros/NULL to the function.
[X] Add more Butcher tables to complete our set for low-order methods.
[X] Remove ark_bd data arrays since we're using a different form of
    dense output.
[X] Code to check Butcher tables -- this should check that ERK
    and IRK tables share root & canopy nodes, that all necessary
    coefficients are defined.  This should be called inside
    ARKInitialSetup.
[X] Create Newton solver for stage systems that uses clearly-named
    arrays, etc., as opposed to shortcut names to unintelligible
    temporary arrays.
[X] Build a new linear solver (ARKLs) to directly tackle the stage
    solves; call this when the user has specified that the system is
    linear (so J only needs to be set up at changes to the time step
    size).  Consider calling this to handle calculation of the updated 
    solution and embedding when M != I (of course, this would mean
    that I only allow linear operators for M, but that's a reasonable
    restriction). 
[X] Code to fill in values for last successful step: told, hold, yold,
    ynew, fold, fnew.  For arrays, use pointer swap to omit need to
    copy values from ynew to yold and fnew to fold.  Still do the copy
    to fill ynew and fnew, since user may input y itself into code,
    and pointer should remain fixed.
[X] ARKCompleteStage routine, to evaluate Fe on current solution and
    perform any other necessary cleanup operations.
[X] ARKPredict2 routine to do stage prediction [to replace ARKPredict]: 
    This can use dense output formulae and/or existing solution values
    for prediction, depending on performance and/or time step sizes.
[X] Consider removing integer parameter ark_smax_alloc, since I can
    always check whether the N_Vector is NULL or not and
    allocate/deallocate appropriately.
[X] Create ARKStep2 routine to emulate ARKStep, repeating the process
    for each internal stage.  Add code to always check the diagonal
    value inside Ai.  If it's zero (ERK or ESDIRK) just declare
    success (unless M != I).  If I just declare success, I should be
    certain to still evaluate Fi at the "solution" of the stage, since
    it would occur already within an implicit solve, and then I can
    just trust that it is still available for subsequent stages.
[X] Create ARKSet2 routine to set up data used in either the explicit
    or implicit RK method for a given stage.  At the ith stage, we  
    compute 
       r = -zi + yn + h*sum_{j=0}^{i-1} Ae(i,j)*Fe(j) 
                    + h*sum_{j=0}^{i} Ai(i,j)*Fi(j)
    so this routine should compute
       gamma = h * Ai(i,i)
       data = yn + h*sum_{j=0}^{i-1} [Ae(i,j)*Fe(j) + Ai(i,j)*Fi(j)]
    and store these in ark_mem->ark_gamma and ark_mem->ark_Fe[i],
    respectively.  All other gamma-related statistics should be
    updated appropriately, as is done with the BDF approach.
[X] Create ARKAdjustParams2 routine to do all of the same things as
    for BDF method, but with all order changes removed.
[X] Revise ARKStep and ARKNlsNewton to account for the fact that we
    now store both ynew and tnew (the last accurate solution and when
    it is valid), so we can just copy those instead of requiring us to
    un-apply modifications that have been made to the last valid
    solution.
[X] Add code to ARKInitialSetup to fill in yold=ynew and fold=fnew
    with values from initial condition so that they can be reused
    later, both for dense output and for calculation of the initial h
    value.
[X] Update ARKHin to use ynew and fnew (already filled in by
    ARKInitialSetup) in estimation of initial time step.
[ ] Check all linear solvers to ensure that they setup/solve the
    correct system, and access all of the correct data out of ark_mem.
[ ] Investigate what is held in ark_mem->ark_tq[4].  This is currently
    used in the iterative linear solver convergence tests, so either I
    need to set the value myself (not so good) or replace what is
    currently there with my own ark_mem->ark_delta value.
[ ] Insert calls to my ARKAdapt function wherever CVODE currently
    decides on accuracy- or stability-based time step adaptivity.
[ ] Think about scenario in which ARK method's first stage doesn't set
    c[0] = 0.0:  I need to fill in fold and fnew for dense output and
    predictors, so if they are already stored in Fe[0],Fi[0] or
    Fe[1],Fi[1] I could just copy them; otherwise I'll need to
    compute them at the end of each successful step.
[ ] Think about scenario where an attempted step has too much error
    (but still converges within iteration limits).  Add code to
    ARKRestore() to recompute predictions for implicit solutions at
    these shorter times. 
[ ] User-defined operator M : R^n -> R^n, and logic to account for
    this in step solution & embedding.  Update time step
    solution/embedding routine to now do solves to compute these.
[ ] Code to allow a change in the Butcher table, both to allow an
    increased/decreased number of stages (no real issue for decrease,
    don't even need to free the N_Vectors if I don't want to; need to
    malloc the new vectors for an increase, though), and to do the
    relevant checking/cleaning of the Butcher table, etc..  This could
    occur only inside ARKodeReInit, since that's the moment where I
    would allow this kind of thing (and would need to reset accuracy
    history data). 
[ ] Go through new and existing routines to determine whether we need
    additional #define control constants, return values and error
    messages; also prune out unused ones.
[ ] Build a Matlab routine to check the analytical order of accuracy
    and B-stability of a generic Butcher table.  Could even have a
    routine that would evaluate the stability function for a variety
    of h*lambda values to test for A-stability (and even L-stability).
[ ] Go through logic of when everything is updated, especially wrt the
    predictor data (and if it's ever called after a failed step).
[ ] Create flowchart of which vectors are used when, and trim down
    memory footprint by reusing some as temporary data.  Then update
    the total real/integer work length estimates in ARKodeCreate.  I
    may even be able to leverage the fact that be = bi, through
    initializing RHS vectors for each stage, and just updating each
    one with the appropriate multiple of fe() and fi(), and then only
    storing (fe() + fi()).  Would this actually save any data storage,
    or just make things more complicated?  It might save a bit for ARK
    methods, but cost a bit more for IRK/ERK methods.  Think about it.
[ ] Ensure that after calling ARKodeReInit (and ark_nst is reset to
    0), that none of the stuff I put into ARKInitialSetup will
    backfire if called again.
[ ] Tighten tolerance for iterative linear solvers when running in
    linearly implicit mode.
[ ] Add a solver option and update linear solve routine for the
    scenario that a problem depends linearly on y, but that the linear 
    dependence changes as a function of time.  In this case, we must
    call lsetup once prior to EVERY time step.
[ ] If I need to use any currently-unused N_Vector operations, add
    them to ARKCheckNvector routine.
[ ] Rename ark_zn[0] to ark_ycur, and remove all references to other
    ark_zn[i].  Also check that I am consistent in which vector is
    chosen as the initial guess for the nonlinear problems.  In some
    places I call this ark_Fi[i] and in others I call this ark_zn[0].
    Settle on something.
[ ] Currently, I use Fe for temporary storage within the solver.
    Unfortunately, this will not exist for an implicit-only problem.
    Figure out a different temporary array to use (preferrably one
    that is unused at these stages of the calculation).


Hard:
[ ] Enable my solver and disable BDF solver.  This will require
    searching through the code for all of the marked locations where I
    made notes to myself (in all caps, and usually preceded by /*****)
[ ] Remove BDF code.
[ ] Implement a new nonlinear solver that can allow for a full Newton
    iteration instead of just the modified Newton iteration currently
    present.
[ ] Look into root-finding stuff.
[ ] Move all fancy logic determination stuff that's currently located
    in specific solve routines (including linear solvers) to reside in
    a self-contained set of functions that ARKode provides.  Attach
    these functions to linear solver data structures where relevant.
[ ] Form corresponding nonlinear solver data structures that will
    contain function pointers for logic arguments, nonlinear residual
    calculations, tolerances, etc., and have Newton method use
    information from that structure instead of calling items that are
    hard-coded into ark_mem.


Unknown/Questions:
[ ] Code for customization of time accuracy norm?  Since SUNDIALS does 
    not implement a general p-norm, I could swap between WRMS and WMax
    norms, though the Wmax norm will require a bit of work (product
    followed by max).  This might break all of our other heuristics,
    especially since the nonlinear/linear solvers all use WRMS, so
    maybe I should NOT do this?
[ ] Code to select the best time step estimation method & parameters
    for a specific method?  I don't actually know what this would be
    for ERK, DIRK, ESDIRK, ARK, etc., but if I did then I should pick
    the best.  Maybe I should build the functionality first, but leave
    it disabled until after I can run exhaustive testing of the full
    suite of implemented methods on a range of test problems?
[ ] Code to perform automated explicit time step stability estimates.
    This could be approximate, using a "random" vector and
    approximating the Jacobian of Fe via finite differencing, and then
    computing the Ritz quotient to estimate the largest eigenvalue.
    Of course, since each ERK has a different stability region, maybe
    some of that information should be included as well?  Also, if Fe
    has complex eigenvalues, the Ritz quotient cannot output complex
    values since it is the ratio of 2 real numbers.  What could I do
    in that scenario?  Need to read up on this kind of thing: has it
    been done before, what did they do, can I do it with the tools at
    my disposal? 
[ ] Consider changing logic for initial timestep estimate, since I
    do not need to start the calculation with a 1st order method and
    bootstrap my way up.
[ ] Consider whether a non-identity matrix M will change how
    preconditioning occurs, i.e. whether it disables either right or
    left preconditioning.
[ ] Update ARKSpilsAtimes function (and similar, e.g. arkDenseSetup) 
    to accommodate a non-identity mass matrix.  This may also change 
    how gamma/gammap are handled, since changes to gamma may have 
    more/less effect with a non-identity mass matrix.  Think about 
    this.
[ ] Consider all the places where I use Fe[] and Fi[] for temporary
    data.  If I'm running a purely implicit or explicit calculation,
    then the other set of these is never allocated!!  Figure out how
    to deal with this situation!
